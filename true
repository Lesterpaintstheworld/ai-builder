import sys
import json
import numpy as np
import struct
import os
import logging
import math
from scipy.spatial.transform import Rotation

# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Add file handler for detailed logging
file_handler = logging.FileHandler('texture_transform.log')
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s')
file_handler.setFormatter(file_formatter)
logger.addHandler(file_handler)

# Add console handler for INFO level logs
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)
logger.addHandler(console_handler)

def apply_texture_transform(uv, transform):
    logger.debug(f"Applying texture transform: {transform}")
    offset = transform.get('offset', [0, 0])
    rotation = transform.get('rotation', 0)
    scale = transform.get('scale', [1, 1])

    # Convert to numpy array if it's not already
    uv = np.array(uv, dtype=np.float32)

    # Apply scale
    uv[0] = uv[0] * scale[0]
    uv[1] = uv[1] * scale[1]

    # Apply rotation
    if rotation != 0:
        cos_rot = math.cos(rotation)
        sin_rot = math.sin(rotation)
        x, y = uv
        uv[0] = x * cos_rot - y * sin_rot
        uv[1] = x * sin_rot + y * cos_rot

    # Apply offset
    uv[0] += offset[0]
    uv[1] += 1 - offset[1]  # Flip Y-axis offset

    logger.debug(f"Transformed UV: {uv}")
    return uv.tolist()

def get_accessor_data(gltf_data, buffer_data, accessor_index):
    logger.debug(f"Getting accessor data for index {accessor_index}")
    accessor = gltf_data['accessors'][accessor_index]
    buffer_view = gltf_data['bufferViews'][accessor['bufferView']]
    
    start = buffer_view['byteOffset'] + accessor.get('byteOffset', 0)
    count = accessor['count']
    component_type = accessor['componentType']
    type = accessor['type']

    logger.debug(f"Accessor details: start={start}, count={count}, component_type={component_type}, type={type}")

    if component_type == 5126:  # FLOAT
        if type == 'VEC2':
            data = np.frombuffer(buffer_data[start:start + count * 8], dtype=np.float32).reshape(-1, 2)
        elif type == 'VEC3':
            data = np.frombuffer(buffer_data[start:start + count * 12], dtype=np.float32).reshape(-1, 3)
    elif component_type == 5123:  # UNSIGNED_SHORT
        data = np.frombuffer(buffer_data[start:start + count * 2], dtype=np.uint16)
    elif component_type == 5125:  # UNSIGNED_INT
        data = np.frombuffer(buffer_data[start:start + count * 4], dtype=np.uint32)
    else:
        raise ValueError(f"Unsupported component type: {component_type}")

    logger.debug(f"Retrieved data shape: {data.shape}")
    return data

def set_accessor_data(gltf_data, buffer_data, accessor_index, data):
    logger.debug(f"Setting accessor data for index {accessor_index}")
    accessor = gltf_data['accessors'][accessor_index]
    buffer_view = gltf_data['bufferViews'][accessor['bufferView']]
    
    start = buffer_view['byteOffset'] + accessor.get('byteOffset', 0)
    buffer_data[start:start + data.nbytes] = data.tobytes()

    # Update accessor min and max
    if accessor['componentType'] in [5126, 5123, 5125]:  # FLOAT, UNSIGNED_SHORT, UNSIGNED_INT
        accessor['min'] = data.min(axis=0).tolist()
        accessor['max'] = data.max(axis=0).tolist()
    
    logger.debug(f"Updated accessor min/max: {accessor['min']} / {accessor['max']}")

def process_gltf(gltf_data, buffer_data):
    scale_factor = 100.0  # Increased to 100 for extreme exaggeration
    logger.info(f"Processing gltf data with scale factor: {scale_factor}")

    for mesh_index, mesh in enumerate(gltf_data['meshes']):
        logger.info(f"Processing mesh {mesh_index}")
        for primitive_index, primitive in enumerate(mesh['primitives']):
            logger.info(f"  Processing primitive {primitive_index}")
            
            # Process vertex positions
            if 'POSITION' in primitive['attributes']:
                position_accessor_index = primitive['attributes']['POSITION']
                positions = get_accessor_data(gltf_data, buffer_data, position_accessor_index)
                
                logger.info(f"    Original positions shape: {positions.shape}")
                logger.info(f"    Original positions min: {positions.min()}, max: {positions.max()}")
                
                # Scale the positions to exaggerate the geometry
                positions *= scale_factor
                
                logger.info(f"    Scaled positions min: {positions.min()}, max: {positions.max()}")
                logger.info(f"    Scaled positions shape: {positions.shape}")
                
                set_accessor_data(gltf_data, buffer_data, position_accessor_index, positions)
                logger.info(f"    Updated position data in gltf_data and buffer_data")

            # Process UV coordinates
            if 'TEXCOORD_0' in primitive['attributes']:
                texcoord_accessor_index = primitive['attributes']['TEXCOORD_0']
                uv_data = get_accessor_data(gltf_data, buffer_data, texcoord_accessor_index)
                logger.info(f"    Original UV data shape: {uv_data.shape}")
                logger.info(f"    Original UV data min: {uv_data.min()}, max: {uv_data.max()}")

                # Check if there's a material with texture transform
                if 'material' in primitive:
                    material = gltf_data['materials'][primitive['material']]
                    if 'extensions' in material and 'KHR_texture_transform' in material['extensions']:
                        transform = material['extensions']['KHR_texture_transform']
                        logger.info(f"    Applying texture transform: {transform}")
                        uv_data = np.array([apply_texture_transform(uv, transform) for uv in uv_data])
                        set_accessor_data(gltf_data, buffer_data, texcoord_accessor_index, uv_data)
                        logger.info(f"    Updated UV data shape: {uv_data.shape}")
                        logger.info(f"    Updated UV data min: {uv_data.min()}, max: {uv_data.max()}")

            # Process indices
            if 'indices' in primitive:
                indices_accessor_index = primitive['indices']
                indices = get_accessor_data(gltf_data, buffer_data, indices_accessor_index)
                logger.info(f"    Indices shape: {indices.shape}")
                logger.info(f"    Indices min: {indices.min()}, max: {indices.max()}")

    # Remove node scaling as we've already scaled the positions
    for node in gltf_data['nodes']:
        if 'scale' in node:
            logger.info(f"Removing scale from node: {node.get('name', 'unnamed')}")
            del node['scale']
        if 'matrix' in node:
            logger.info(f"Converting matrix to TRS for node: {node.get('name', 'unnamed')}")
            matrix = np.array(node['matrix']).reshape(4, 4)
            translation = matrix[:3, 3]
            rotation = Rotation.from_matrix(matrix[:3, :3])
            node['translation'] = translation.tolist()
            node['rotation'] = rotation.as_quat().tolist()
            del node['matrix']

    # Remove KHR_texture_transform from extensionsUsed and extensionsRequired
    for ext_list in ['extensionsUsed', 'extensionsRequired']:
        if ext_list in gltf_data:
            gltf_data[ext_list] = [ext for ext in gltf_data[ext_list] if ext != 'KHR_texture_transform']
            logger.info(f"Removed KHR_texture_transform from {ext_list}")

    logger.info("GLTF data processing completed")
    return gltf_data, buffer_data

def bake_texture_transform(input_file, output_file):
    logger.info(f"Processing file: {input_file}")
    # Read the GLB file
    with open(input_file, 'rb') as f:
        data = f.read()

    logger.info(f"File size: {len(data)} bytes")

    # Extract the JSON chunk
    magic = data[:4]
    if magic != b'glTF':
        raise ValueError("Invalid GLB file: magic number not found")

    version, length = struct.unpack('<II', data[4:12])
    if version != 2:
        raise ValueError(f"Unsupported GLB version: {version}")

    chunk_length, chunk_type = struct.unpack('<II', data[12:20])
    if chunk_type != 0x4E4F534A:  # JSON chunk type
        raise ValueError("First chunk is not JSON")

    json_data = data[20:20+chunk_length]
    
    logger.info(f"JSON chunk size: {len(json_data)} bytes")

    # Parse the JSON data
    gltf_data = json.loads(json_data)

    # Extract the binary buffer data
    buffer_start = 20 + chunk_length
    chunk_length, chunk_type = struct.unpack('<II', data[buffer_start:buffer_start+8])
    if chunk_type != 0x004E4942:  # BIN chunk type
        raise ValueError("Second chunk is not BIN")

    buffer_data = bytearray(data[buffer_start+8:buffer_start+8+chunk_length])

    # Log initial mesh data
    log_mesh_data(gltf_data, buffer_data, "Before processing")

    # Process the glTF data
    gltf_data, buffer_data = process_gltf(gltf_data, buffer_data)

    # Log processed mesh data
    log_mesh_data(gltf_data, buffer_data, "After processing")

    # Convert the modified JSON back to bytes
    modified_json = json.dumps(gltf_data, separators=(',', ':')).encode('utf-8')

    logger.info(f"Modified JSON chunk size: {len(modified_json)} bytes")

    # Pad the JSON to maintain 4-byte alignment
    padding = (4 - (len(modified_json) % 4)) % 4
    modified_json += b' ' * padding

    # Calculate the total file length
    total_length = 12 + 8 + len(modified_json) + 8 + len(buffer_data)

    # Construct the GLB file
    glb_header = struct.pack('<4sII', b'glTF', 2, total_length)
    json_header = struct.pack('<II', len(modified_json), 0x4E4F534A)  # 'JSON' in little endian
    bin_header = struct.pack('<II', len(buffer_data), 0x004E4942)  # 'BIN\0' in little endian

    # Write the new GLB file
    with open(output_file, 'wb') as f:
        f.write(glb_header)
        f.write(json_header)
        f.write(modified_json)
        f.write(bin_header)
        f.write(buffer_data)

    logger.info(f"Written output to: {output_file}")

    # Verify the output file
    verify_glb_file(output_file)

def verify_glb_file(file_path):
    logger.info(f"Verifying GLB file: {file_path}")
    with open(file_path, 'rb') as f:
        data = f.read()

    # Check magic number
    if data[:4] != b'glTF':
        logger.error("Invalid GLB file: magic number not found")
        return

    # Check version
    version = struct.unpack('<I', data[4:8])[0]
    if version != 2:
        logger.error(f"Unsupported GLB version: {version}")
        return

    # Check total length
    total_length = struct.unpack('<I', data[8:12])[0]
    if total_length != len(data):
        logger.error(f"GLB file length mismatch. Header: {total_length}, Actual: {len(data)}")
        return

    # Check JSON chunk
    json_length, json_type = struct.unpack('<II', data[12:20])
    if json_type != 0x4E4F534A:
        logger.error("First chunk is not JSON")
        return

    # Check BIN chunk
    bin_start = 20 + json_length + (4 - (json_length % 4)) % 4
    bin_length, bin_type = struct.unpack('<II', data[bin_start:bin_start+8])
    if bin_type != 0x004E4942:
        logger.error("Second chunk is not BIN")
        return

    logger.info("GLB file structure verified successfully")

def log_mesh_data(gltf_data, buffer_data, stage):
    logger.info(f"Mesh data {stage}:")
    for mesh_index, mesh in enumerate(gltf_data['meshes']):
        for primitive_index, primitive in enumerate(mesh['primitives']):
            if 'POSITION' in primitive['attributes']:
                position_accessor_index = primitive['attributes']['POSITION']
                positions = get_accessor_data(gltf_data, buffer_data, position_accessor_index)
                logger.info(f"  Mesh {mesh_index}, Primitive {primitive_index}:")
                logger.info(f"    Positions shape: {positions.shape}")
                logger.info(f"    Positions min: {positions.min()}, max: {positions.max()}")

def main():
    if len(sys.argv) != 3:
        logger.error("Usage: python script.py <input_file> <output_file>")
        sys.exit(1)

    input_file = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.exists(input_file):
        logger.error(f"Error: Input file '{input_file}' does not exist.")
        sys.exit(1)

    logger.info(f"Starting texture transform baking process for {input_file}")
    bake_texture_transform(input_file, output_file)
    logger.info(f"Texture transform baking process completed. Output saved to {output_file}")

if __name__ == "__main__":
    main()
